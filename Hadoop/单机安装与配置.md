## Hadoop单机配置

#### 1、运行平台

* Linux或windows平台均可。但主流平台以Linux为主。

#### 2、必需软件

* jdk1.8
* hadoop2.8.4
* ssh
* rsync

#### 3、Hadoop配置

* hadoop-env.sh：修改JAVA_HOME为系统jdk的安装目录，***export $JAVA_HOME = /usr/java/latest***

---------------------------------------local模式-------------------------------------------

* core-site.xml：

  ```shell
  <configuration>
  	<property>
  		<name>fs.defaultFS</name>
  		<value>hdfs://localhost:9000</value>
  	</property>
  </configuration>
  ```

* hdfs-site.xml:

  ```shell
  <configuration>
  	<property>
  		<name>dfs.replication</name>
  		<value>1</value>
  	</property>
  </configuration>
  ```

* 免密登录：
  * 生成密钥：***ssh-keygen -t rsa -p '' -f ~/.ssh/id_rsa***
  * 将公钥发给自己：***cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys*** 或 ***ssh-copy-id destIP***
  * 修改文件权限：***chmod 0600 ~/.ssh/authorized_keys***

> ** 任务执行流程：
>
> * 初始化文件系统：***hadoop namenode -format***
>
> * 启动集群：***start-dfs.sh***
>
> * 在hdfs上创建任务目录：***bin/hdfs dfs -mkdir -p /user/<username>***
>
> * 拷贝输入文件到hdfs：***bin/hdfs dfs -put source hdfsDir***
>
> * 提交任务测试结果：
>
>   ```shell
>   bin/hadoop jarshare/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar grep input output 'dfs[a-z.]+'
>   ```

* 单节点yarn配置：

  * mapred-site.xml：

    ```shell
    <configuration>
    	<property>
    		<name>mapreduce.framework.name</name>
    		<value>yarn</value>
    	</property>
    </configuration>
    ```

  * yarn-site.xml：

    ```shell
    <configuration>
    	<property>
    		<name>yarn.nodemanager.aux-services</name>
    		<value>mapreduce_shuffle</value>
    	</property>
    </configuration>
    ```

--------------------------------------------------伪分布式--------------------------------------------------------

